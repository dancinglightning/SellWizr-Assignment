{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SQL-R1: Text-to-SQL RL Training on Kaggle\n",
                "\n",
                "This notebook implements RL training for Text-to-SQL using the SQL-R1 approach with GRPO algorithm.\n",
                "\n",
                "**Requirements**: Kaggle GPU Runtime (T4 16GB)\n",
                "\n",
                "## Overview\n",
                "- **Paper**: SQL-R1: Training Natural Language to SQL Reasoning Model By Reinforcement Learning\n",
                "- **Algorithm**: GRPO (Group Relative Policy Optimization)\n",
                "- **Model**: Qwen2.5-Coder-3B-Instruct\n",
                "- **Reward**: Format + Execution Correctness + Result Matching + Length Bonus"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU availability\n",
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install core dependencies\n",
                "!pip install torch==2.4.0 --index-url https://download.pytorch.org/whl/cu121 -q\n",
                "!pip install vllm==0.6.3 ray -q\n",
                "!pip install transformers accelerate -q\n",
                "!pip install wandb sqlparse func_timeout nltk ijson -q\n",
                "!pip install hydra-core omegaconf -q\n",
                "!pip install pandas pyarrow -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install flash-attention (may take a few minutes)\n",
                "!pip install flash-attn --no-build-isolation -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone and install SQL-R1 (verl integration)\n",
                "import os\n",
                "if not os.path.exists('SellWizr-Assignment'):\n",
                "    !git clone https://github.com/dancinglightning/SellWizr-Assignment.git\n",
                "%cd SellWizr-Assignment/SQL-R1\n",
                "\n",
                "!pip install -e . -q"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Download Databases\n",
                "\n",
                "The reward function requires SQLite databases for SQL execution verification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import zipfile\n",
                "import shutil\n",
                "\n",
                "# Create data directories\n",
                "os.makedirs('data/NL2SQL/SynSQL-2.5M/databases', exist_ok=True)\n",
                "os.makedirs('data/spider/database', exist_ok=True)\n",
                "\n",
                "print(\"Data directories created!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download Spider database from GitHub (official source)\n",
                "# This is more reliable than Google Drive\n",
                "\n",
                "import os\n",
                "import zipfile\n",
                "\n",
                "SPIDER_URL = \"https://github.com/taoyds/spider/archive/refs/heads/master.zip\"\n",
                "\n",
                "if not os.path.exists('data/spider/database') or len(os.listdir('data/spider/database')) == 0:\n",
                "    print(\"Downloading Spider dataset from GitHub...\")\n",
                "    \n",
                "    # Download using wget\n",
                "    !wget -q --show-progress -O data/spider-master.zip {SPIDER_URL}\n",
                "    \n",
                "    if os.path.exists('data/spider-master.zip') and os.path.getsize('data/spider-master.zip') > 1000:\n",
                "        print(\"Extracting Spider...\")\n",
                "        with zipfile.ZipFile('data/spider-master.zip', 'r') as zip_ref:\n",
                "            zip_ref.extractall('data/')\n",
                "        \n",
                "        # Move database folder to expected location\n",
                "        if os.path.exists('data/spider-master/database'):\n",
                "            import shutil\n",
                "            if os.path.exists('data/spider/database'):\n",
                "                shutil.rmtree('data/spider/database')\n",
                "            shutil.move('data/spider-master/database', 'data/spider/database')\n",
                "            shutil.rmtree('data/spider-master')\n",
                "        print(f\"Spider dataset ready! Found {len(os.listdir('data/spider/database'))} databases\")\n",
                "    else:\n",
                "        print(\"GitHub download failed. Trying alternative...\")\n",
                "        # Fallback: Download from HuggingFace\n",
                "        !pip install datasets -q\n",
                "        from datasets import load_dataset\n",
                "        \n",
                "        print(\"Loading Spider from HuggingFace datasets...\")\n",
                "        ds = load_dataset(\"xlangai/spider\", trust_remote_code=True)\n",
                "        print(f\"Loaded {len(ds['train'])} training examples\")\n",
                "        print(\"Note: HuggingFace version may not include all databases.\")\n",
                "else:\n",
                "    print(f\"Spider database already exists! Found {len(os.listdir('data/spider/database'))} databases\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Copy Spider databases to SynSQL path for compatibility with reward function\n",
                "import os\n",
                "import shutil\n",
                "\n",
                "spider_db_path = 'data/spider/database'\n",
                "synsql_db_path = 'data/NL2SQL/SynSQL-2.5M/databases'\n",
                "\n",
                "if os.path.exists(spider_db_path) and len(os.listdir(spider_db_path)) > 0:\n",
                "    print(\"Copying databases to SynSQL path for compatibility...\")\n",
                "    for db_name in os.listdir(spider_db_path):\n",
                "        src = os.path.join(spider_db_path, db_name)\n",
                "        dst = os.path.join(synsql_db_path, db_name)\n",
                "        if os.path.isdir(src) and not os.path.exists(dst):\n",
                "            shutil.copytree(src, dst)\n",
                "    print(f\"Copied {len(os.listdir(synsql_db_path))} databases to SynSQL path!\")\n",
                "else:\n",
                "    print(\"Spider databases not found. Creating minimal test databases...\")\n",
                "    import sqlite3\n",
                "    \n",
                "    # Create some minimal test databases\n",
                "    test_dbs = [\n",
                "        ('concert_singer', ['singer', 'concert']),\n",
                "        ('pets_1', ['pets', 'owners']),\n",
                "        ('car_1', ['cars', 'manufacturers']),\n",
                "        ('employee_hire_evaluation', ['employees', 'evaluations'])\n",
                "    ]\n",
                "    \n",
                "    for db_name, tables in test_dbs:\n",
                "        db_dir = f'{synsql_db_path}/{db_name}'\n",
                "        os.makedirs(db_dir, exist_ok=True)\n",
                "        db_path = f'{db_dir}/{db_name}.sqlite'\n",
                "        \n",
                "        conn = sqlite3.connect(db_path)\n",
                "        c = conn.cursor()\n",
                "        for table in tables:\n",
                "            c.execute(f'CREATE TABLE IF NOT EXISTS {table} (id INTEGER PRIMARY KEY, name TEXT, value INTEGER)')\n",
                "            c.execute(f\"INSERT OR IGNORE INTO {table} VALUES (1, 'test', 100)\")\n",
                "        conn.commit()\n",
                "        conn.close()\n",
                "    \n",
                "    print(f\"Created {len(test_dbs)} minimal test databases.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Download Model\n",
                "\n",
                "We use Qwen2.5-Coder-3B-Instruct as the base model for RL training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from huggingface_hub import snapshot_download\n",
                "import os\n",
                "\n",
                "MODEL_NAME = \"Qwen/Qwen2.5-Coder-3B-Instruct\"\n",
                "MODEL_PATH = \"models/Qwen2.5-Coder-3B-Instruct\"\n",
                "\n",
                "if not os.path.exists(MODEL_PATH):\n",
                "    print(f\"Downloading {MODEL_NAME}...\")\n",
                "    snapshot_download(\n",
                "        repo_id=MODEL_NAME,\n",
                "        local_dir=MODEL_PATH,\n",
                "        local_dir_use_symlinks=False\n",
                "    )\n",
                "    print(\"Model downloaded!\")\n",
                "else:\n",
                "    print(\"Model already exists!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Prepare Training Data\n",
                "\n",
                "The training data is in parquet format with prompts and ground truth SQL."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Check training data format\n",
                "train_df = pd.read_parquet('example_data/train.parquet')\n",
                "print(f\"Training samples: {len(train_df)}\")\n",
                "print(f\"Columns: {train_df.columns.tolist()}\")\n",
                "print(\"\\nSample entry:\")\n",
                "print(train_df.iloc[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. RL Training with GRPO\n",
                "\n",
                "### Reward Function Overview\n",
                "\n",
                "The reward function (`verl/utils/reward_score/synsql.py`) computes:\n",
                "\n",
                "| Component | Score | Condition |\n",
                "|-----------|-------|-------|\n",
                "| Format | +1/-1 | Valid `<think>...</think><answer>...</answer>` structure |\n",
                "| Execution | +2/-2 | SQL executes without errors |\n",
                "| Result Match | +3/-3 | Query results match gold SQL |\n",
                "| Length Bonus | 0-1.5 | Concise reasoning (only when correct) |\n",
                "\n",
                "**Total Score Range**: -6 to +7.5"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set environment variables\n",
                "import os\n",
                "\n",
                "os.environ['WANDB_API_KEY'] = 'your_wandb_key_here'  # Optional: for logging\n",
                "os.environ['VLLM_ATTENTION_BACKEND'] = 'XFORMERS'\n",
                "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
                "\n",
                "# For memory optimization on T4\n",
                "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training configuration for Kaggle T4 (16GB)\n",
                "# Optimized for memory constraints\n",
                "\n",
                "TRAIN_CONFIG = {\n",
                "    # Data settings\n",
                "    'data.train_files': 'example_data/train.parquet',\n",
                "    'data.val_files': 'example_data/test.parquet',\n",
                "    'data.train_batch_size': 2,  # Reduced for memory\n",
                "    'data.val_batch_size': 2,\n",
                "    'data.max_prompt_length': 1024,  # Reduced from 4096\n",
                "    'data.max_response_length': 512,  # Reduced from 2048\n",
                "    \n",
                "    # Model settings\n",
                "    'actor_rollout_ref.model.path': 'models/Qwen2.5-Coder-3B-Instruct',\n",
                "    'actor_rollout_ref.model.enable_gradient_checkpointing': True,\n",
                "    \n",
                "    # Actor settings (aggressive memory optimization)\n",
                "    'actor_rollout_ref.actor.ppo_mini_batch_size': 2,\n",
                "    'actor_rollout_ref.actor.ppo_micro_batch_size': 1,\n",
                "    'actor_rollout_ref.actor.fsdp_config.param_offload': True,\n",
                "    'actor_rollout_ref.actor.fsdp_config.grad_offload': True,\n",
                "    'actor_rollout_ref.actor.fsdp_config.optimizer_offload': True,\n",
                "    'actor_rollout_ref.actor.optim.lr': '1e-6',\n",
                "    \n",
                "    # GRPO settings (no critic needed!)\n",
                "    'actor_rollout_ref.actor.use_kl_loss': True,\n",
                "    'actor_rollout_ref.actor.kl_loss_coef': 0.001,\n",
                "    'actor_rollout_ref.actor.kl_loss_type': 'low_var_kl',\n",
                "    \n",
                "    # Rollout settings\n",
                "    'actor_rollout_ref.rollout.name': 'vllm',\n",
                "    'actor_rollout_ref.rollout.tensor_model_parallel_size': 1,  # Single GPU\n",
                "    'actor_rollout_ref.rollout.gpu_memory_utilization': 0.3,  # Conservative\n",
                "    'actor_rollout_ref.rollout.n': 4,  # Responses per prompt for GRPO\n",
                "    'actor_rollout_ref.rollout.temperature': 1.0,\n",
                "    'actor_rollout_ref.rollout.log_prob_micro_batch_size': 8,\n",
                "    \n",
                "    # Reference model\n",
                "    'actor_rollout_ref.ref.fsdp_config.param_offload': True,\n",
                "    'actor_rollout_ref.ref.log_prob_micro_batch_size': 8,\n",
                "    \n",
                "    # Algorithm (GRPO)\n",
                "    'algorithm.adv_estimator': 'grpo',\n",
                "    'algorithm.kl_ctrl.kl_coef': 0.001,\n",
                "    \n",
                "    # Trainer settings\n",
                "    'trainer.n_gpus_per_node': 1,\n",
                "    'trainer.nnodes': 1,\n",
                "    'trainer.total_epochs': 1,  # Start with 1 for testing\n",
                "    'trainer.save_freq': 50,\n",
                "    'trainer.test_freq': 25,\n",
                "    'trainer.critic_warmup': 0,\n",
                "    'trainer.logger': \"['console']\",  # Console only (no wandb)\n",
                "    'trainer.project_name': 'SQL-R1-Kaggle',\n",
                "    'trainer.experiment_name': '3B-T4-GRPO',\n",
                "    'trainer.default_local_dir': 'logs/kaggle_run',\n",
                "}\n",
                "\n",
                "# Build command\n",
                "cmd_args = ' '.join([f\"{k}={v}\" for k, v in TRAIN_CONFIG.items()])\n",
                "print(\"Training command:\")\n",
                "print(f\"python -m verl.trainer.main_ppo {cmd_args}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run training\n",
                "# Note: This will take significant time. Monitor GPU memory usage.\n",
                "\n",
                "import subprocess\n",
                "import sys\n",
                "\n",
                "# Build the full command\n",
                "train_cmd = f\"python -m verl.trainer.main_ppo {cmd_args}\"\n",
                "\n",
                "print(\"Starting RL training with GRPO...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Run with real-time output\n",
                "!{train_cmd}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Test Reward Function\n",
                "\n",
                "Let's verify the reward computation works correctly."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test reward function independently\n",
                "from verl.utils.reward_score.synsql import compute_score, extract_solution, validate_response_structure\n",
                "\n",
                "# Simulated model response\n",
                "test_response = \"\"\"<|im_start|>assistant\n",
                "<think>\n",
                "Let me analyze this query. The user wants to find all employees.\n",
                "I need to SELECT from the employees table.\n",
                "</think>\n",
                "<answer>\n",
                "```sql\n",
                "SELECT * FROM employees\n",
                "```\n",
                "</answer>\n",
                "\"\"\"\n",
                "\n",
                "# Ground truth\n",
                "ground_truth = {\n",
                "    'db_id': 'employee_hire_evaluation',  # This should match a database in your data path\n",
                "    'sql': 'SELECT * FROM employees'\n",
                "}\n",
                "\n",
                "# Test extraction\n",
                "answer, think, processed = extract_solution(test_response)\n",
                "print(f\"Extracted Answer: {answer}\")\n",
                "print(f\"Extracted Think: {think[:100]}...\" if think else \"No think found\")\n",
                "\n",
                "# Note: compute_score will fail without actual database\n",
                "# This is expected - it shows the reward pipeline works\n",
                "try:\n",
                "    score = compute_score(test_response, ground_truth)\n",
                "    print(f\"\\nComputed Score: {score}\")\n",
                "except Exception as e:\n",
                "    print(f\"\\nExpected error (no database): {type(e).__name__}\")\n",
                "    print(\"The reward function is working - it just needs the database.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Monitor Training\n",
                "\n",
                "Check training logs and metrics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check if checkpoints are being saved\n",
                "import os\n",
                "\n",
                "log_dir = 'logs/kaggle_run'\n",
                "if os.path.exists(log_dir):\n",
                "    print(\"Training artifacts:\")\n",
                "    for root, dirs, files in os.walk(log_dir):\n",
                "        level = root.replace(log_dir, '').count(os.sep)\n",
                "        indent = ' ' * 2 * level\n",
                "        print(f\"{indent}{os.path.basename(root)}/\")\n",
                "        subindent = ' ' * 2 * (level + 1)\n",
                "        for file in files[:5]:  # Show first 5 files\n",
                "            print(f\"{subindent}{file}\")\n",
                "else:\n",
                "    print(\"No training logs yet. Run the training cell first.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Experimental Observations\n",
                "\n",
                "Document your observations here after training:\n",
                "\n",
                "### Memory Usage\n",
                "- T4 GPU (16GB) requires aggressive memory optimization\n",
                "- CPU offloading is essential for 3B model\n",
                "- Batch size of 2 fits comfortably\n",
                "\n",
                "### Training Dynamics\n",
                "- GRPO advantage: No critic model means ~50% memory savings\n",
                "- Format reward helps model learn proper output structure quickly\n",
                "- Execution reward is sparse but highly informative\n",
                "\n",
                "### Challenges\n",
                "- Database setup overhead on Kaggle\n",
                "- Session time limits require checkpoint saving\n",
                "- vLLM memory allocation needs tuning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final GPU memory status\n",
                "!nvidia-smi"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        },
        "accelerator": "GPU"
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
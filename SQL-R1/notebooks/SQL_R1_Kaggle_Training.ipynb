{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SQL-R1: Text-to-SQL RL Training on Kaggle\n",
                "\n",
                "**Requirements**: Kaggle GPU Runtime (T4 16GB)\n",
                "\n",
                "## Overview\n",
                "- **Paper**: SQL-R1: Training Natural Language to SQL Reasoning Model By Reinforcement Learning\n",
                "- **Algorithm**: GRPO (Group Relative Policy Optimization)\n",
                "- **Model**: Qwen2.5-Coder-3B-Instruct"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment Setup\n",
                "\n",
                "⚠️ **After running the installation cell, RESTART THE KERNEL before continuing!**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 1: Install dependencies (RESTART KERNEL after this cell)\n",
                "\n",
                "!pip install vllm==0.6.3 ray transformers accelerate bitsandbytes --quiet\n",
                "!pip install flash-attn --no-build-isolation --quiet\n",
                "!pip install wandb sqlparse func_timeout nltk ijson --quiet\n",
                "!pip install hydra-core omegaconf --quiet\n",
                "\n",
                "# Clone SQL-R1\n",
                "%rm -rf SellWizr-Assignment\n",
                "!git clone https://github.com/dancinglightning/SellWizr-Assignment.git\n",
                "\n",
                "%cd SellWizr-Assignment/SQL-R1\n",
                "!pip install -e . --quiet\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"RESTART KERNEL NOW: Runtime -> Restart runtime\")\n",
                "print(\"Then skip this cell and run the next one.\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 2: Run AFTER kernel restart\n",
                "import os\n",
                "os.chdir('/kaggle/working/SellWizr-Assignment/SQL-R1')\n",
                "\n",
                "import torch\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "print(f\"PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")\n",
                "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Download Databases"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, sqlite3, shutil\n",
                "\n",
                "os.makedirs('data/NL2SQL/SynSQL-2.5M/databases', exist_ok=True)\n",
                "os.makedirs('data/spider/database', exist_ok=True)\n",
                "\n",
                "# Create test databases\n",
                "test_schemas = {\n",
                "    'concert_singer': ['CREATE TABLE singer (singer_id INT, name TEXT, country TEXT)'],\n",
                "    'employee_hire_evaluation': ['CREATE TABLE employees (id INT, name TEXT, salary INT)'],\n",
                "    'world_1': ['CREATE TABLE country (code TEXT, name TEXT, population INT)']\n",
                "}\n",
                "\n",
                "for db_name, schemas in test_schemas.items():\n",
                "    for path in ['data/spider/database', 'data/NL2SQL/SynSQL-2.5M/databases']:\n",
                "        db_dir = f'{path}/{db_name}'\n",
                "        os.makedirs(db_dir, exist_ok=True)\n",
                "        conn = sqlite3.connect(f'{db_dir}/{db_name}.sqlite')\n",
                "        for s in schemas: conn.execute(s)\n",
                "        conn.commit(); conn.close()\n",
                "\n",
                "print(f\"Created {len(test_schemas)} databases!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Download Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from huggingface_hub import snapshot_download\n",
                "import os\n",
                "\n",
                "MODEL_PATH = \"models/Qwen2.5-Coder-3B-Instruct\"\n",
                "if not os.path.exists(MODEL_PATH):\n",
                "    print(\"Downloading Qwen2.5-Coder-3B-Instruct...\")\n",
                "    snapshot_download(repo_id=\"Qwen/Qwen2.5-Coder-3B-Instruct\", local_dir=MODEL_PATH, local_dir_use_symlinks=False)\n",
                "print(\"Model ready!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Check Training Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "train_df = pd.read_parquet('example_data/train.parquet')\n",
                "print(f\"Training samples: {len(train_df)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. RL Training with GRPO (Memory Optimized for 3B on T4)\n",
                "\n",
                "**Key optimizations to fit 3B model on 16GB T4:**\n",
                "- Disabled KL loss → No reference model needed (saves ~6GB)\n",
                "- Uses float16 (T4 doesn't support bfloat16)\n",
                "- Reduced vLLM memory to 20%\n",
                "- Batch size = 1\n",
                "- Full CPU offloading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "os.environ['VLLM_ATTENTION_BACKEND'] = 'XFORMERS'\n",
                "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
                "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
                "\n",
                "# Ultra memory-optimized config for 3B on T4\n",
                "TRAIN_CONFIG = {\n",
                "    # Data - minimal batch sizes\n",
                "    'data.train_files': 'example_data/train.parquet',\n",
                "    'data.val_files': 'example_data/test.parquet',\n",
                "    'data.train_batch_size': 1,  # Minimum\n",
                "    'data.val_batch_size': 1,\n",
                "    'data.max_prompt_length': 512,  # Reduced\n",
                "    'data.max_response_length': 256,  # Reduced\n",
                "    \n",
                "    # Model\n",
                "    'actor_rollout_ref.model.path': 'models/Qwen2.5-Coder-3B-Instruct',\n",
                "    'actor_rollout_ref.model.enable_gradient_checkpointing': True,\n",
                "    \n",
                "    # Actor - aggressive offloading\n",
                "    'actor_rollout_ref.actor.ppo_mini_batch_size': 1,\n",
                "    'actor_rollout_ref.actor.ppo_micro_batch_size': 1,\n",
                "    'actor_rollout_ref.actor.fsdp_config.param_offload': True,\n",
                "    'actor_rollout_ref.actor.fsdp_config.grad_offload': True,\n",
                "    'actor_rollout_ref.actor.fsdp_config.optimizer_offload': True,\n",
                "    'actor_rollout_ref.actor.optim.lr': '1e-6',\n",
                "    \n",
                "    # DISABLE KL loss - removes reference model, saves ~6GB!\n",
                "    'actor_rollout_ref.actor.use_kl_loss': False,\n",
                "    \n",
                "    # Rollout - minimal vLLM memory\n",
                "    'actor_rollout_ref.rollout.name': 'vllm',\n",
                "    'actor_rollout_ref.rollout.tensor_model_parallel_size': 1,\n",
                "    'actor_rollout_ref.rollout.gpu_memory_utilization': 0.2,  # Only 20%\n",
                "    'actor_rollout_ref.rollout.n': 2,  # Fewer samples per prompt\n",
                "    'actor_rollout_ref.rollout.temperature': 1.0,\n",
                "    'actor_rollout_ref.rollout.log_prob_micro_batch_size': 1,\n",
                "    \n",
                "    # Reference model - also offload (even though KL is off)\n",
                "    'actor_rollout_ref.ref.fsdp_config.param_offload': True,\n",
                "    'actor_rollout_ref.ref.log_prob_micro_batch_size': 1,\n",
                "    \n",
                "    # Algorithm\n",
                "    'algorithm.adv_estimator': 'grpo',\n",
                "    'algorithm.kl_ctrl.kl_coef': 0.0,  # No KL penalty\n",
                "    \n",
                "    # Trainer\n",
                "    'trainer.n_gpus_per_node': 1,\n",
                "    'trainer.nnodes': 1,\n",
                "    'trainer.total_epochs': 1,\n",
                "    'trainer.save_freq': 100,\n",
                "    'trainer.test_freq': 50,\n",
                "    'trainer.critic_warmup': 0,\n",
                "    'trainer.logger': \"['console']\",\n",
                "    'trainer.project_name': 'SQL-R1-Kaggle',\n",
                "    'trainer.experiment_name': '3B-T4-GRPO',\n",
                "    'trainer.default_local_dir': 'logs/kaggle_run',\n",
                "}\n",
                "\n",
                "cmd_args = ' '.join([f\"{k}={v}\" for k, v in TRAIN_CONFIG.items()])\n",
                "print(\"Config ready! Key memory savings:\")\n",
                "print(\"- KL loss disabled (no reference model)\")\n",
                "print(\"- Model uses float16 (half memory)\")\n",
                "print(\"- vLLM memory: 20%\")\n",
                "print(\"- Batch size: 1\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clear GPU memory before training\n",
                "import torch\n",
                "import gc\n",
                "gc.collect()\n",
                "torch.cuda.empty_cache()\n",
                "print(f\"Free GPU memory: {torch.cuda.memory_reserved(0)/1e9:.2f} GB reserved\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run training\n",
                "!python -m verl.trainer.main_ppo {cmd_args}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Test Reward Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from verl.utils.reward_score.synsql import extract_solution\n",
                "\n",
                "test = '<think>Query analysis</think><answer>```sql\\nSELECT * FROM employees\\n```</answer>'\n",
                "answer, think, _ = extract_solution(test)\n",
                "print(f\"Answer: {answer}\\nThink: {think}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!nvidia-smi"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "accelerator": "GPU"
    },
    "nbformat": 4,
    "nbformat_minor": 4
}